{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5197 2022 S2 Assignment - Covers the lecture and tutorial materials up to, and including, week 9\n",
    "\n",
    "**SPECIAL NOTE:** Please refer to the [assessment page](https://lms.monash.edu/mod/assign/view.php?id=10554982) for rules, general guidelines and marking rubrics of the assessment (the marking rubric for the kaggle competition part will be released near the deadline in the same page). Failure to comply with the provided information will result in a deduction of mark (i.e. late penalties) or breach of academic integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAME**: Tsz Yan CHUNG\n",
    "\n",
    "**STUDENT ID**: 32973381\n",
    "\n",
    "**KAGGLE NAME/ID** (Your name as it appears on the Kaggle leaderboard, See part 5, Question 8):\n",
    "\n",
    "Please also enter your details in this [google form](https://forms.gle/TsjvDvCMF4Xghknv6).\n",
    "\n",
    "Use of latex is compulsory. Save time and use [mathpix](https://mathpix.com/) to convert your hand-written math to elegant latex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The year is 3022 and Professor Ozstraya’s AI reincarnation has recruited you into the Koala Academy for Statistical Enlightenment with the mission of improving intergalactic joy. To do this you need to understand the statistics of joy and what influences joy. In particular, humans are having a hard time getting on with the killer drop bears of the recently discovered planet, Terra Australis. If you can understand what makes the killer drop bears joyful we may just have a chance at achieving intergalactic joy. \n",
    "\n",
    "We want to know if the drop bears on Terra Australis are joyful or not and how we can make them more joyful. We’ve managed to get a sample of individual joy level from drop bears on the moon of Terra Australis, but we can’t measure the joy of drop bears on Terra Australis directly because they keep it a secret and their defences are too strong. We know they keep a collection of data about themselves that we might be able to use to predict their joy on Terra Australis (not its moon because the bears on the moon could potentially be different in nature to the bears on Terra Australis - we just don't know).  \n",
    "\n",
    "Professor Ozstraya’s AI reincarnated wants to send a mission, called The Endeavour, to get this critical data from Terra Australis to be able to predict joy there as well as understand the other factors influencing their joy. However, there is a danger that if the true mean of average joy on Terra Australis is above 120 then the humans will just want to stay there and not return with the data. Or if it is below 4 then the humans will just get eaten and not be able to return with the data either.\n",
    "\n",
    "**DISCLAIMER:** The story is for illustration purpose only, it is not required to understand the story to solve for each question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Point Estimation (15 marks)\n",
    "\n",
    "To start out you decide you want to model the ‘joy level of an individual drop bear’ $X$ with a distribution. You have your small sample of joy levels of drop bears from the moon of Terra Australis and decide to use maximum likelihood estimation (MLE) to create models of individual joy level using the distributions of joy obtained from three well studied planets: Earth, Kangaroonus and echidnator. You need to determine the MLE estimates for these three distributions and plug in your sample of joy levels of drop bears from the moon of Terra Australis to obtain three different models for the ‘joy level of an individual drop bear’ $X$.  \n",
    "\n",
    "The sample of joy levels you obtained is\n",
    "$$\n",
    "S_1=\\{22.6, 29.1, 8.7, 24.3, 21.5, 13.4, 17.8, 21.7, 37.8, 33.8\\}\n",
    "$$\n",
    "\n",
    "Although this corresponds to a sample size of $n=10$ you should assume for the moment we care about the general case where the sample data has been collected from $n$ [i.i.d](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) random variables $\\mathbf{X}=$ $\\left(X_{1}, \\ldots, X_{n}\\right)$. We want to model these random variables using the following distributions.\n",
    "\n",
    "**WARNING:** you should strictly follow the 3-steps strategy as detailed in [question 2 of week 5 tutorial](https://lms.monash.edu/mod/resource/view.php?id=10555100) \\(or any answer formats presented in the [Week 5 quiz](https://lms.monash.edu/mod/resource/view.php?id=10555092)\\) to answer for the questions that are related to MLE estimators presented in this part. Any deviations from the answer format might result in a loss of marks! \n",
    "\n",
    "You've forgotten how to do MLE and so Professor Ozstraya's AI decides to give you a head start by giving you an example of what is expected by solving the MLE solution for the planet Earth's joy distibution as follows:\n",
    "\n",
    "Assume that Joy on Earth follows distribution A with the following PDF\n",
    "$$\n",
    "f(x \\mid \\theta)=\\frac{1}{\\theta}, \\quad 0 \\leq x \\leq \\theta ; \\quad \\theta>0.\n",
    "$$\n",
    "\n",
    "(a) The maximum likelihood estimator for the parameter $\\theta$, i.e., $\\hat{\\theta}_{\\text{MLE}}$ can be obtained as follows:\n",
    "\n",
    "The likelihood is\n",
    "$$\n",
    "f(\\textbf{x} \\mid \\theta)=f(x_1,x_2,...,x_n \\mid \\theta)=\\prod_{i=1}^n f(x_i \\mid \\theta) = \\left(\\frac{1}{\\theta}\\right)^n.\n",
    "$$\n",
    "The negative log likelihood is \n",
    "$$\n",
    "L(\\textbf{x} \\mid \\theta)=-\\log(f(\\textbf{x} \\mid \\theta)) = -\\log\\left(\\left(\\frac{1}{\\theta}\\right)^n\\right) = n\\log(\\theta).\n",
    "$$\n",
    "When we attempt to find the stationary point of the negative log likelihood we find that the derivative with respect to $\\theta$ is\n",
    "$$\n",
    "\\frac{dL(\\textbf{x} \\mid \\theta)}{d\\theta}=\\frac{dn\\log(\\theta)}{d\\theta}=\\frac{n}{\\theta}.\n",
    "$$\n",
    "We can't set this to zero to find the stationary point and solve for the estimate of $\\theta$ because there is no stationary point. Instead we have to recognise that all of our realisations $x_1,x_2,...,x_n$ will be less than or equal to $\\theta$ according to the PDF definition. This is equivalent to saying $\\theta \\geq x_1,x_2,...,x_n$. This means the negative log likelihood is actually defined with a specific domain as follows:\n",
    "The negative log likelihood is \n",
    "$$\n",
    "L(\\textbf{x} \\mid \\theta)= n\\log(\\theta), \\quad \\theta \\geq x_1,x_2,...,x_n.\n",
    "$$\n",
    "This is an increasing function with positive slope because $n$ and $\\theta$ are positive and so the derivative of the function is also positive. We can also note the domain of this function starts at $\\theta$ equal to the maximum value of $x_1,x_2,...,x_n$ since $\\theta \\geq x_1,x_2,...,x_n$. Therefore, since this is an increasing function starting from $\\theta$ equal to the maximum value of $x_1,x_2,...,x_n$, the minimum of this function occurs when $\\theta$ equals the maximum value of $x_1,x_2,...,x_n$. Therefore we let the MLE estimate be $\\hat{\\theta}=\\text{argmax}_{x_i}\\{x_1,x_2,...,x_n\\}$. (Draw a diagram of the negative log likelihood if you don't get it.)\n",
    "\n",
    "(b) Now using our sample of drop bear joy $S_1$ given above we can note that the MLE estimate in this case becomes\n",
    "$$\n",
    "\\hat{\\theta}=\\text{argmax}_{x_i}\\{x_1,x_2,...,x_n\\}=\\text{argmax}_{x_i}\\{22.6, 29.1, 8.7, 24.3, 21.5, 13.4, 17.8, 21.7, 37.8, 33.8\\}=37.8.\n",
    "$$\n",
    "We can then use this value to define our plug-in distribution A to model individual joy of drop bears as\n",
    "$$\n",
    "f(x \\mid \\hat{\\theta}=37.8)=\\frac{1}{37.8}, \\quad 0 \\leq x \\leq 37.8\n",
    "$$\n",
    "Now it is your turn to solve the MLE solution for the joy distibutions for Kangaroonus and Echidnator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (7.5 marks)\n",
    "\n",
    "Assume that Joy on Kangaroonus follows distribution B:\n",
    "$$\n",
    "f(x \\mid \\theta)=\\frac{\\theta}{x^{1+\\theta}}, \\quad 1 \\leq x<\\infty ; \\quad \\theta>0.\n",
    "$$\n",
    "\n",
    "(a) Find the maximum likelihood estimator for the parameter $\\theta$, i.e., $\\hat{\\theta}_{\\text{MLE}}$. (3.5 marks)\n",
    "\n",
    "(b) Using our sample of drop bear joy $S_1$ given above find the MLE estimate in this case and obtain the plug-in distribution. (0.5 marks)\n",
    "\n",
    "(c) You are not sure about the correctness of your MLE estimate. Instead of using MLE, you decide to use the sample mean as the estimator for $\\theta$, is the sample mean a biased or unbiased estimator? (3.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "(a)\n",
    "\n",
    "Given \n",
    "$$f(x \\mid \\theta)=\\frac{\\theta}{x^{1+\\theta}}, \\quad 1 \\leq x<\\infty ; \\quad \\theta>0$$\n",
    "and n independent and identical distributed random variable $\\textbf{X}$ = $\\left(x_1, x_2, ..., x_n\\right)$, a maximum likelihood estimation model could be set up as below:\n",
    "\n",
    "Step 1. Set up likelihood function \n",
    "$$f(\\textbf{X} \\mid \\theta)=f\\left(x_1 \\ldots x_n\\mid\\theta\\right)$$\n",
    "$$=\\prod_{i=1}^n f\\left(x_i \\mid \\theta\\right)$$\n",
    "$$=\\prod_{i = 1}^n \\frac{\\theta}{x_i^{1+\\theta}}$$\n",
    "$$=\\frac{\\theta^n}{\\prod_{i=1}^n x_i^{1+\\theta}}$$\n",
    "Step 2. Find negative log likelihood by taking negative logarithm of the function\n",
    "\n",
    "$$L(x \\mid \\theta)$$\n",
    "$$=-\\ln f(x \\mid \\theta)=-\\ln \\frac{\\theta^n}{\\prod_{i=1}^n x_i^{1+\\theta}}$$\n",
    "$$=-n \\ln \\theta-\\ln \\prod_{i=1}^n x_i^{-(1+\\theta)}$$\n",
    "$$=-n \\ln \\theta+(1+\\theta)\\sum_{i=1}^n\\ln x_i$$\n",
    "\n",
    "\n",
    "Step 3.\n",
    "$$\\frac{dL(x \\mid \\theta)}{d\\theta} = 0$$\n",
    "$$-\\frac{n}{\\theta} + \\sum_{i=1}^n\\ln x_i= 0$$\n",
    "$$\\hat\\theta_{MLE} = \\frac{n}{\\sum_{i=1}^n\\ln x_i}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "\n",
    "with:\n",
    "$$S_1=\\{22.6, 29.1, 8.7, 24.3, 21.5, 13.4, 17.8, 21.7, 37.8, 33.8\\}$$\n",
    "\n",
    "$$\\hat\\theta_{MLE} = \\frac{10}{\\ln 22.6 + \\ln29.1 + \\ln8.7 + \\ln24.3+\\ln21.5+\\ln13.4+\\ln17.8+\\ln21.7+\\ln37.8+\\ln33.8}$$\n",
    "\n",
    "$$\\hat\\theta_{MLE} = 0.326636$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)\n",
    "\n",
    "The sample mean could be computed as:\n",
    "$$ E[\\hat\\theta] = \\frac{1}{n}{\\sum_{i=1}^n} x_i$$\n",
    "\n",
    "As for the population mean, it could be computed by integrating the PDF given:\n",
    "$$E[\\theta] = \\int_1^\\infty x f(x|\\theta) dx$$\n",
    "\n",
    "$$=\\int_1^\\infty x\\frac{\\theta}{x^{1+\\theta}} dx$$\n",
    "$$=\\int_1^\\infty \\frac{\\theta}{x^{\\theta}} dx$$\n",
    "$$= \\left[ \\theta \\cdot \\frac{x^{-\\theta +1}}{1-\\theta} \\right]^{\\infty}_1$$\n",
    "$$= \\left[ \\frac{1}{(x^{\\theta -1})} \\cdot \\frac{\\theta}{1-\\theta} \\right]^{\\infty}_1$$\n",
    "$$= 0 - \\frac{\\theta}{1-\\theta}$$\n",
    "$$= \\frac{\\theta}{\\theta -1}$$\n",
    "As\n",
    "$$ E[\\theta] \\neq E[\\hat \\theta]$$\n",
    "It can be conclude that the sample mean is a biased estimator of the population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (7.5 marks)\n",
    "\n",
    "Joy on Echidnater follows distribution C:\n",
    "\n",
    "$$\n",
    "f(x \\mid \\alpha, \\gamma)=\\frac{\\gamma x^{\\gamma-1}}{\\alpha^\\gamma} \\exp \\left[-\\left(\\frac{x}{\\alpha}\\right)^\\gamma\\right] \\text { for } x>0 ; \\quad \\alpha, \\gamma>0\n",
    "$$\n",
    "\n",
    "Please\n",
    "\n",
    "(a) Find maximum likelihood estimator for the parameter $\\alpha$, assuming $\\gamma$ is a constant and not a parameter. (3.5 marks)\n",
    "\n",
    "(b) Assuming $\\gamma=1$ and using our sample of drop bear joy $S_1$ given above find the MLE estimate in this case and obtain the plug-in distribution. (0.5 marks)\n",
    "\n",
    "(c) If you are going to use a small sample pf data to calculate an estimate of $\\alpha$ to specify a single plug-in distribution, explain whether you can do it using the form you got in (a). (3.5 marks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "(a):\n",
    "\n",
    "Step 1.$\\\\$\n",
    "With n independent and identical distributed random variable $\\textbf{X}$ = $\\left(x_1, x_2, ..., x_n\\right)$, a maximum likelihood estimation model could be set up as below:\n",
    "$$f(x \\mid \\alpha, \\gamma)$$\n",
    "$$= \\prod_{i=1}^n f(x\\mid\\alpha, \\gamma)$$\n",
    "\n",
    "Step 2. \n",
    "$$L(x\\mid \\alpha) = -\\ln f(x \\mid \\alpha, \\gamma)$$\n",
    "$$=-\\ln \\prod_{i=1}^n\\left[\\frac{\\gamma x_i^{\\gamma-1}}{\\alpha^\\gamma} \\exp \\left[-\\left(\\frac{x_i}{\\alpha}\\right)^\\gamma\\right]\\right]$$\n",
    "$$=-\\sum_{i=1}^n\\left[\\ln \\left(\\frac{\\gamma x_i^{\\gamma-1}}{a^\\gamma}\\right)-\\left(\\frac{x_i}{\\alpha}\\right)^\\gamma\\right]$$\n",
    "$$=-n \\ln \\gamma+\\gamma n \\ln \\alpha- \\sum_{i=1}^n \\left[\\left(\\gamma -1\\right)\\ln x_i - \\left(\\frac{x_i}{\\alpha}\\right)^\\gamma\\right]$$\n",
    "$$= -n \\ln \\gamma+\\gamma n \\ln \\alpha - \\left(\\gamma -1\\right)\\sum_{i = 1}^n\\ln x_i + \\sum_{i=1}^n\\left(\\frac{x_i}{\\alpha}\\right)^\\gamma$$\n",
    "\n",
    "Step 3.\n",
    "$$\n",
    "\\frac{d L(x \\mid \\alpha)}{d \\alpha} = \\frac{\\gamma n}{\\alpha}+\\sum_{i =1}^n  x_i^\\gamma \\left(\\frac{-\\gamma}{a^{\\gamma+1}}\\right)=0$$\n",
    "$$\\frac{\\gamma n}{\\alpha}-\\frac{\\gamma}{a^{\\gamma+1}} \\sum_{i=1}^n x_i^\\gamma  =0$$\n",
    "$$\\frac{\\gamma n}{\\alpha}=\\frac{\\gamma}{a^{\\gamma+1}} \\sum_{i=1}^n x_i^\\gamma$$\n",
    "$$\\frac{\\gamma n}{\\alpha}=\\frac{\\gamma}{a^{\\gamma+1}} \\sum_{i=1}^n x_i^\\gamma$$\n",
    "$$\\frac{\\alpha^{\\gamma +1}}{\\alpha} = \\frac{\\gamma}{\\gamma n} \\sum_{i=1}^n x_i^\\gamma$$\n",
    "$${\\alpha^\\gamma}= \\frac{1}{n} \\sum_{i=1}^n x_i^\\gamma$$\n",
    "$$\\alpha = (\\frac{1}{n} \\sum_{i=1}^n x_i^\\gamma)^{\\frac{1}{\\gamma}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "Substitute $\\gamma = 1$:\n",
    "$$\\alpha = (\\frac{1}{n} \\sum_{i=1}^n x_i)$$\n",
    "With:\n",
    "$$S_1=\\{22.6, 29.1, 8.7, 24.3, 21.5, 13.4, 17.8, 21.7, 37.8, 33.8\\}$$\n",
    "$$\\alpha = \\frac{1}{10} \\left(22.6+29.1+8.7+24.3+21.5+13.4+17.8+21.7+37.8+33.8\\right)$$\n",
    "$$\\alpha = 23.07$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)\n",
    "\n",
    "\n",
    "There are two major reasons that the form obtained in part a might not be the best estimator.\n",
    "$\\\\$\n",
    "First, given the sample is small, maximum likelihood estimation (MLE) may induce a large value of bias. Under MLE, the ground truth of alpha is computed as $\\alpha = \\hat \\alpha_{MLE} + e$, where $e$ indicate biasness of the estimator, which could be represented as: $$e = \\frac{\\sigma}{\\sqrt n}$$ \n",
    "\n",
    "From the equation, biasness is inversely related to the size of sample. If the sample is large, bias will tend to be small. Hence, estimates are closed to the true values. On the other hand, if the sample size is small, bias tends to be big, which may lead to inaccurate estimate values.\n",
    "\n",
    "Secondly, when applying maximum likelihood estimation, it is assumed that the probabilities of realizing different value of $x$ are independent, so that we could set up the equation in the first step of part a answers, that:\n",
    "$$f(x \\mid \\alpha, \\gamma) = \\prod_{i=1}^n f(x\\mid\\alpha, \\gamma) $$\n",
    "However, consider $\\alpha$ as a parameter of $x$ (joy level), the true values of joy level are not limit to 4 to 120. The realized range of 4 to 120 was simply because other values are unmeasurable due to the fact that the human carrying out mission with these measurements were unable to return the measured result. And thus, it is likely that the probabilities of realizing different value of $x$ are NOT independent in this case, which violated the assumption of MLE, and make MLE unusable.\n",
    "\n",
    "An extra point to add is that $\\gamma$ was assumed to be a constant and non-parameter of $x$. However, it is uncertain if such assumption is realistic based on the forementioned reason about the dependency of measured $x$ value and unmeasurable values. If $\\gamma$ turns out to be a parameter of $x$, our estimator obtained in part a will be unreliable given more than one violated assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Simulation (10 marks)\n",
    "\n",
    "After constructing the plug-in models using MLE you realise that what you actually need is a model of the average joy level of drop bears on Terra Australis to be able to have an impact on all the bears on the planet. This is because if you know the average joy is pretty high you can be confident the drop bears will become friends with humans.\n",
    "\n",
    "Professor Ozstraya's AI reminds you about the central limit theorem (CLT) and how you can use it to \n",
    "obtain a sampling distribution of the mean in the form of a Gaussian distribution. You realise this can be used to obtain a model of average joy level on a planet. Rather than apply the CLT to all of the plug-in distributions for individual joy level you obtained in Part 1 above, you decide to apply it to the plug-in distribution from Earth in the solution provided by Professor Ozstraya's AI.    \n",
    "\n",
    "Rather than trust the CLT you also want to verify it's correctness through simulation so you can feel safe in assuming average joy level on a planet can be modelled using a Gaussian distribution. \n",
    "\n",
    "**DISCLAIMER:** The story is for illustration purpose only, it is not required to understand the story to solve for each question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following experimental design definitions:\n",
    "\n",
    "$\\textbf{simulations}$: Number of samples you repeatedly take - for all $\\textbf{Part 2, Q2}$ we set this number equal to $10000$, i.e., you have $10000$ samples. If you have trouble understanding this, perhaps it is time to rewatch the lecture recordings/materials.\n",
    "\n",
    "$\\textbf{n}$: Number of observations per sample, this will be given in the question as we will experiment with different values of $\\textbf{n}$.\n",
    "\n",
    "$\\textbf{PDF(X)}$: Is the probability density function that the random variable $X$ follows (please check Lecture 2 and Tutorial 2).\n",
    "\n",
    "$\\textbf{Random Variables RVs}\\;\\;$ $X_{1},X_{2},\\ldots,X_{n}\\sim\\operatorname{PDF}(\\textbf{X})\\;:\\;\\;$ All the random variables in the sample (observation RVs) will follow the distribution set out by the PDF. Again, the number of observations $\\textbf{n}$ as well as the distribution $\\textbf{PDF(X)}$ has not been set here but will be given in the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Theoretical Set-up for the CLT (No Coding or Simulation here!) (2 Marks)\n",
    "\n",
    "Before simulating the CLT, we must first establish what we would want to see from the simulation a.k.a what the theory tells us. Thus, we are going to set up the experiment here as well as set up our expectation for the $\\textbf{(1) Summation Distribution}\\;\\sum_{i}^{n}X_{i}\\;$, and $\\textbf{(2) Mean Distribution}\\;\\overline{X}\\equiv\\frac{\\sum_{i}^{n}X_{i}}{n}\\;$.\n",
    "\n",
    "We will consider ``one`` set-up for $\\textbf{PDF(X)}$ which we set to be the plug-in distribution A \n",
    "$$\n",
    "f(x \\mid \\hat{\\theta}=37.8)=\\frac{1}{37.8}, \\quad 0 \\leq x \\leq 37.8.\n",
    "$$\n",
    "We can note this is actually the uniform distribution $\\operatorname{U}(0, 37.8)$.\n",
    "\n",
    "Additionally, we will also consider ``three`` different values for $\\textbf{n}$, namely $\\;\\operatorname{n}_{\\operatorname{Small}} = 10\\;$, $\\;\\operatorname{n}_{\\operatorname{Medium}} = 30\\;$, $\\;\\operatorname{n}_{\\operatorname{Big}} = 100\\;$.\n",
    "\n",
    "Simply, we would like to obtain the distribution for $\\textbf{(1)}$ and $\\textbf{(2)}$ for each value of $\\textbf{n}$ and the $\\textbf{PDF(X)}$ that we set here. $\\;$ Again, please revisit the lecture and tutorial materials if you have doubts. Please put down your result up to five decimal places as we would like to compare this result with the simulation results later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "For the PDF(X):\n",
    "\n",
    "$$\n",
    "f(x \\mid \\hat{\\theta}=37.8)=\\frac{1}{37.8}, \\quad 0 \\leq x \\leq 37.8.\n",
    "$$\n",
    "The expected value $\\mu$ and variance $\\sigma^2$ could be sought as follows: \n",
    "$$\n",
    "\\mu = E[X] = \\frac{(0 + 37.8)}{2} = 18.9\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2 = Var(X) = \\frac{(37.8 - 0)^2}{12} = 119.07\n",
    "$$ \n",
    "\n",
    "\n",
    "For a uniform distribution, according to the Central Limit Theorem (CLT), when number of observation is large enough, the distribution will be converted to a normal distribution. \n",
    "\n",
    "For (1) summation distribution $\\sum_{i=1}^n X_i$, the distribution under CLT would be:\n",
    "$$\\sum_{i=1}^n X_i \\sim N(n\\mu, n\\sigma^2)$$\n",
    "when n = 10:\n",
    "\n",
    "$$\\sum_{i=1}^n X_i \\sim N(n_{Small}\\mu, n_{Small}\\sigma^2)$$\n",
    "$$=\\sum_{i=1}^n X_i \\sim N(189, 1190.7)$$\n",
    "\n",
    "when n = 30:\n",
    "$$\\sum_{i=1}^n X_i \\sim N(n_{Medium}\\mu, n_{Medium}\\sigma^2)$$\n",
    "$$=\\sum_{i=1}^n X_i \\sim N(567, 3572.01)$$\n",
    "when n = 100:\n",
    "$$\\sum_{i=1}^n X_i \\sim N(n_{Big}\\mu, n_{Big}\\sigma^2)$$\n",
    "$$=\\sum_{i=1}^n X_i \\sim N(1890, 11907)$$\n",
    "\n",
    "For (2) Mean distribution $\\overline{X}\\equiv\\frac{\\sum_{i}^{n}X_{i}}{n}$, the distribution under CLT would be:\n",
    "$$\\overline{X}\\equiv\\frac{\\sum_{i}^{n}X_{i}}{n} \\sim N(\\mu, \\frac{\\sigma^2}{n})$$\n",
    "when n = 10:\n",
    "$$\\overline{X}\\equiv\\frac{\\sum_{i}^{n}X_{i}}{n} \\sim N(\\mu, \\frac{\\sigma^2}{n_{Small}})$$\n",
    "$$\\overline{X}\\equiv\\frac{\\sum_{i}^{n}X_{i}}{n} \\sim N(18.9, 11.907)$$\n",
    "when n = 30:\n",
    "$$\\overline{X}\\equiv\\frac{\\sum_{i}^{n}X_{i}}{n} \\sim N(\\mu, \\frac{\\sigma^2}{n_{Medium}})$$\n",
    "$$\\overline{X}\\equiv\\frac{\\sum_{i}^{n}X_{i}}{n} \\sim N(18.9, 3.969)$$\n",
    "when n = 100:\n",
    "$$\\overline{X}\\equiv\\frac{\\sum_{i}^{n}X_{i}}{n} \\sim N(\\mu, \\frac{\\sigma^2}{n_{Big}})$$\n",
    "$$\\overline{X}\\equiv\\frac{\\sum_{i}^{n}X_{i}}{n} \\sim N(18.9, 1.1907)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Simulating the CLT result (NO LIBRARIES ALLOWED) (8 Marks)\n",
    "\n",
    "After finishing $\\textbf{Question 1}$, you should have the theoretical results. In this question, you will use these theoretical results to compare with the simulation results and verify the CLT. As you should know by now, the CLT is based on the ideas of repeated sampling, so please simulate your results accordingly under the ``one`` given $\\textbf{PDF(X)}$ and the ``three`` sample sizes $\\textbf{n}$ for the ``two`` distributions $\\textbf{(1)}$ and $\\textbf{(2)}$, the number of combinations is the same with $\\textbf{question 1}$ - since we would like to compare simulations with theoretical values.\n",
    "\n",
    "For each combination of $\\textbf{n}$ and $\\textbf{PMF(Y)}$ under each distribution $\\textbf{(1)}$ and $\\textbf{(2)}$, you are required to display a ``histogram`` to represent the results of repeated sampling, and a ``curve`` to display the theoretical results from $\\textbf{Question 1}$. Explain your findings and results (no more than ``150`` words).\n",
    "\n",
    "$\\textbf{Instructions for plots (MUST FOLLOW)}$: The marking for this question also includes the cleanliness of your plots (proper labels for axes, name of the plot must include the type of sampling distribution, and the sample size that you are using, e.g. ``Mean Distribution: n = 30``). The theoretical values and simulated values need to be presented accordingly for ease of comparison - you must put these values in the legends.\n",
    "\n",
    "$\\textbf{Instructions for R code (MUST FOLLOW)}$: The code needs to be elegant ($\\textbf{do not hard code}$) with enough comments describing what you want to do. Furthermore, the naming of the variables needs to make sense. If you need to use a chunk of code for more than one time, please write a function for it, we will $\\textbf{deduct marks}$ if you copy and paste your codes here and there. As specified from the beginning, please put your result with 5 decimal places so we can compare and assess the theoretical results of the CLT and its simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER BLOCK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots <- function(observations,  observation_size, summation = TRUE){\n",
    "    samples <- c()\n",
    "    for(i in 1:10000){\n",
    "        samples <- append(samples, ifelse(summation, sum(samples(observations, size = observation_size, replace = TRUE)))\n",
    "                        , mean((samples(observations, size = observation_size, replace = TRUE))))\n",
    "    }\n",
    "    mean_real  <- ifelse(summation, observation_size * A, A)\n",
    "    variance_real  <- ifelse(summation, observation_size * B, B)\n",
    "    \n",
    "    hist(samples, col = 'blue', freq = FALSE, nclass = observation_size/2)\n",
    "    lines(range, dnorm(range,mean = mean_real, \n",
    "                      sd = sqrt(variance_real)), col = \"pink\", lwd = 3)\n",
    "    line(density(samples), lwd = 3, col = 'red')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations <- runif(10000, min = 0, max = 37.8)\n",
    "observation_size  <- c(10, 30, 100)\n",
    "whole  <- par()\n",
    "options(repr.plit.width = 10, repr.plot.height = 10, repr.plot.res = 160)\n",
    "par(mfrow = c(3,2))\n",
    "\n",
    "for (n in observation_size){\n",
    "    plots(observations, observation_size = n, summation = TRUE)\n",
    "    plots(observations, observation_size = n, summation = TRUE)  \n",
    "}\n",
    "\n",
    "par(whole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Hypothesis Testing (5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Warning__: If it is not explicitly stated, please assume the 5% significance level.\n",
    "\n",
    "After simulating and verifying the CLT you feel ok about choosing to model the average joy level on a planet using a Gaussian distribution. While you have been working hard another returning mission has acquired a sample of average joy values from the moon of Terra Australis.\n",
    "\n",
    "**DISCLAIMER:** The story is for illustration purpose only, it is not required to understand the story to solve for each question\n",
    "\n",
    "## Question 1 (2.5 Marks)\n",
    "Assume the average joy of killer drop bears on a planet follows a Gaussian distribution. For the moment treat the new sample of average joy values from the bears on the moon of Terra Australis as being representative of the bears on the planet Terra Australis. Note the new sample is as follows:\n",
    "$$\n",
    "S_2=\\{2.7, 6.2, 5.3, 3.6, 4.8, 5.1, 4.5, 1.6, 0.9, 2.2\\}\n",
    "$$\n",
    "Current research defines a species having an average joy under 4 as a \"joyless\" species. Using the new sample $S_2$ please test the hypothesis that the killer drop bears are a \"joyless\" species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "\n",
    "Set up hypothesis:\n",
    "$$H_0: \\mu \\geq 4$$\n",
    "$$H_A: \\mu < 4$$\n",
    "From the sample, the expecpted value $\\mu_{S_2}$ and the variacne $\\sigma_{S_2}$could be calculated as follows:\n",
    "\n",
    "$$\\mu_{S_2} = \\frac{\\sum_{i=1}^nS_{2,i}}{n}$$\n",
    "where $i$ represents the index of observations, and n is the number of observations in the sample\n",
    "\n",
    "$$\\mu_{S_2} = 3.69$$\n",
    "\n",
    "$$\\sigma_{S_2}^2 = \\frac{n}{n-1}\\left(E[X^2] - {E[X]}^2\\right)$$\n",
    "$$\\sigma_{S_2}^2 = \\frac{10}{9}\\left(\\frac{164.29}{10} - {3.69}^2\\right)$$\n",
    "$$\\sigma_{S_2}^2 = 3.1254$$\n",
    "$$\\sigma_{S_2} = 1.7679$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, it is obtained that\n",
    "$$\\mu_{S_2} = 3.69$$\n",
    "$$\\sigma_{S_2} = 1.7679$$\n",
    "\n",
    "In this case, population variance is unknown, a t-test distribution should be applied.\n",
    "At 5% significance level with degree of freedom of 9, the critical value could be represented as $t_{0.05, 9}$, given that this is an one-sided test.\n",
    "\n",
    "Apply test statistics:\n",
    "$$T = \\frac{\\mu_{S_2} - \\mu}{{\\sigma_{S_2}} / {\\sqrt(n)}}$$\n",
    "$$T = \\frac{3.69 - 4}{{1.7679}/\\sqrt(10)}$$\n",
    "$$T = -0.554$$\n",
    "while $t_{0.05, 9} = 1.833$ according to the t-distribution table.\n",
    "Since $|T| < t_{0.05, 9}$, there is no enough evidence to reject the null, and thus there is no enough evidence to state that Killer Drop Bear is a joyless species.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (2.5 Marks)\n",
    "Along with the new sample obtained from the moon of Terra Australis, the mission team brought back a hologram of a killer drop bear. When you took a glance, a weird idea crossed your mind. Is it possible that the killer drop bears are actually koalas? You started searching for information and found a paper published 800 years ago. The paper claimed the joy of koalas has dropped a lot in the last decade, saying \"5 out of the 20 tested groups of koalas have been shown to be joyless\". They have classified a koala group as \"joyless\" if they have a an average joy score under 4.\n",
    "\n",
    "Test the hypothesis that the probability of a \"joyless\" group of killer drop bears is equal to the probability of a \"joyless\" group of koalas. Please convert the sample of average joy values $S_2$ in question 1 to binary data with a threshold of 4, test the hypothesis, and interpret the p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "Given sample 2\n",
    "$$\n",
    "S_2=\\{2.7, 6.2, 5.3, 3.6, 4.8, 5.1, 4.5, 1.6, 0.9, 2.2\\}\n",
    "$$\n",
    "Under a threshold of 4, it could be convert into a string of binary number, where 0 represets an animal is joyless, while 1 indicates it is not joyless:\n",
    "$$\n",
    "S_3 = \\{0,1,1,0,1,1,1,0,0,0\\}\n",
    "$$\n",
    "\n",
    "It could be considered as a Bernoulli distribution, with the random variable $S_3$ distributed as follows:\n",
    "$$\\theta_{S_3} = E[S_3] = 0.5$$\n",
    "$$S_3 \\sim Be(0.5)$$\n",
    "Hence, a hypothese test could be set up:\n",
    "$$H_0: \\theta = \\frac{1}{4}$$\n",
    "$$H_A: \\theta \\neq \\frac{1}{4}$$\n",
    "\n",
    "For a Bernoulli distribution, the following test-statistic could be applied:\n",
    "$$ Z = \\frac{\\theta_{S_3} - \\theta_0}{\\sqrt{\\theta_0 (1- \\theta_0)/n}}$$\n",
    "For the sample given, number of observation n is 10\n",
    "$$ Z = \\frac{0.5 - \\frac{1}{4}}{\\sqrt{\\frac{1}{4} (1- \\frac{1}{4})/10}}$$\n",
    "$$ Z = 1.8257$$\n",
    "$$P(-1.8257 < Z < +1.8257) = P(Z < 1.8257) - P(Z < -1.8257)$$\n",
    "$$P(-1.8257 < Z < +1.8257) = 0.9328$$\n",
    "\n",
    "Hence, the p-value of the test could be computed as $1-0.9328 = 0.0672 > 0.05$.\n",
    "Therefore, there is no enough evidence to reject the null. We cannot reject the hypothesis that Killer Drop Bear and Koala are the same type of animal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Confidence Interval Estimation & Central Limit Theorem (20 marks)\n",
    "**WARNING:** If it is not explicitly stated, please assume the 95% confidence.\n",
    "\n",
    "Based on your hypothesis testing results you decide it might be a good idea to model killer drop bears as koalas from Earth. You decide to study the joy of koala colonies from around Earth. If you can construct a confidence interval that suggests the true mean of average joy of koala colonies is between 4 and 120, suggesting drop bear colony average joy will also be in this range, then Professor Ozstraya's AI will launch The Endeavour mission to collect the critical data we need to predict average joy on Terra Australis. To do this you need examine different ways of determining the number of koalas in a colony and the number of koala colonies you need to sample to get the desired parameters of The Endeavour mission. This will in turn be used to help us determine the number of drop bear colonies that need to be sampled to the get the desired parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (5 marks)\n",
    "\n",
    "To get a single estimate of the probability of koalas being joyful, Professor Ozstraya’s AI reincarnated will use the Koala Reader camera to say if they are joyful or not. After getting the data, you will take the average joyfulness of all koalas captured by camera as an estimate $\\hat p$ for the true probability of koalas being joyful $p$.  Now it's necessary to ensure that there is at least 99% certainty that the difference between the empirical probability $\\hat p$ and the actual probability $p$ is not more than 5%. What is the minimum number of koalas in a colony that should be investigated using the Koala Reader camera? \n",
    "\n",
    "Please use the central limit theorem to answer this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "Let $\\textbf X$ be the random variable of the observation on whether a koala is joyful or not, which 1 represent a Koala is joyful while 0 represent a Koala is joyless.\n",
    "$$\\textbf X \\sim Be(\\hat p)$$\n",
    "A confidence interval could be establish as follows:\n",
    "$$\\left(\\hat p - Z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{\\hat p (1-\\hat p)}{n}}, \\hat p + Z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{\\hat p (1-\\hat p)}{n}}\\right)$$\n",
    "Where $\\alpha$ is the significance level, and n is the number of koalas needed for investigation.\n",
    "Now, to ensure the estimate $\\hat p$ is no more than 5% away from the true probability $p$, $Z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{\\hat p (1-\\hat p)}{n}}$ could be represented as the distance of $\\hat p$ is away from $p$.\n",
    "To limit the distance to 5%:\n",
    "$$Z_{\\frac{\\alpha}{2}}\\sqrt{\\frac{\\hat p (1-\\hat p)}{n}} \\leq 0.05$$\n",
    "At a 99% significance level, $\\alpha = 0.01$. Given that it is an one-tail inequality, $Z_{0.01} = 2.33$ by referencing the Z-table\n",
    "$$2.33\\sqrt{\\frac{\\hat p (1-\\hat p)}{n}} \\leq 0.05$$\n",
    "$$n \\geq \\frac{2.33^2}{0.05^2} \\hat p (1-\\hat p)$$\n",
    "\n",
    "Since $\\hat p$ is unknown, to ensure enough observations were made, $\\hat p (1-\\hat p)$ is assumed to be at its maximum, that is:\n",
    "$$\\frac{d\\hat p (1-\\hat p)}{d\\hat p} = 0$$\n",
    "$$\\hat p = 0.5$$\n",
    "Hence, value of n could be obtained by the equation:\n",
    "$$n \\geq \\frac{2.33^2}{0.05^2} 0.5(1-0.5)$$\n",
    "$$n \\geq 542.89$$\n",
    "\n",
    "To conclude, 543 Koalas in a colony will need to be investigated, in order to be 99% certain that difference between the estimate $\\hat p$ is no more than 5% away from the true value $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (5 marks)\n",
    "\n",
    "Professor Ozstraya’s AI reincarnated tells you that a recent global study has shown the average joy of koala colonies on Earth follows a N($19$, $16$) distribution according to CLT. You believe him because its consistent with your simulations of the CLT relating to average joy above.\n",
    "\n",
    "Accordingly, you are going to take the $\\mu = 19$ and $\\sigma = 16$ to build a final confidence interval for the true mean of the average joy of koala colonies's. If you want to have that 95% confidence interval nicely located inside the interval [4, 120], what is the minimum number of koala colonies you need to sample from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "Under a normal distribution N(19, 16), with variance now known as $\\sigma^2 = 16^2$, confidence interval could be set up as:\n",
    "$$(\\bar X - Z_{\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt n}, \\bar X + Z_{\\frac{\\alpha}{2}}\\frac{\\sigma}{\\sqrt n})$$\n",
    "To have 95% confidence interval that locates inside the interval [4, 120], i.e. $\\alpha = 0.05$ and $Z_{\\frac{\\alpha}{2}} = 1.96$.\n",
    "\n",
    "With a distribution of $N(\\mu = 19, \\sigma = 16)$, the interval of $[4,120]$ are: $\\frac{4}{16} = 0.25\\sigma$ and $\\frac{120}{16}\\sigma = 7.5\\sigma$ away from the mean respectively.\n",
    "\n",
    "The confidence interval could be set up as:\n",
    "$$(\\bar X - Z_{\\frac{\\alpha}{2}}\\frac{\\sigma_{lower}}{\\sqrt n}, \\bar X + Z_{\\frac{\\alpha}{2}}\\frac{\\sigma_{upper}}{\\sqrt n})$$\n",
    "$$(19 - 1.96\\frac{0.25\\sigma}{\\sqrt n}, 19 + 1.96\\frac{7.5\\sigma}{\\sqrt n})$$ \n",
    "\n",
    "For interval to fit within the range of 4 to 120:\n",
    "$$\\left ( 19 + 1.96\\frac{7.5\\sigma}{\\sqrt n}\\right ) - \\left(19 - 1.96\\frac{0.25\\sigma}{\\sqrt n} \\right ) \\leq (120 - 4)$$\n",
    "\n",
    "$$\\frac{1}{\\sqrt{n}} \\left [ 1.96(7.5\\sigma) + 1.96(0.25\\sigma) \\right] \\leq 116$$\n",
    "$$ n \\geq \\left[\\frac{1.96(7.5)(16) + 1.96(0.25)(16)}{116}\\right]^2$$\n",
    "\n",
    "$$ n \\geq 4.38$$\n",
    "\n",
    "The minimum number of koala colonies need to be sampled is $5$, in order to be $95%$ confident that the sample will be located within the interval $[4, 120]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (5 marks)\n",
    "Professor Ozstraya’s AI holds some doubts about your 95% confidence interval result so he wants to ask you to explain the meaning of confidence intervals.\n",
    "\n",
    "His question is, if you can repeat the whole procedure to generate several confidence intervals, how many of them would you expect to be \"correct\"? By \"correct\" he means the confidence interval actually contains the true population mean. For example, if you estimated 20 confidence intervals, should you be suprised if 16 of them are correct? And why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "For the definition of 95% confidence interval, it means that there will be a probability of 95% that the sample falls within the interval, and 5% that it falls outside of the interval.\n",
    "\n",
    "Hence, a random variable $X$ could be used to represents the the distribution of whether the samples fall within the interval as:\n",
    "$$ X \\sim Be(0.95)$$\n",
    "which 1 represents the sample falls within the interval, and 0 represents otherwise.\n",
    "Under CLT, $X$ will be under a normal distribution $Y$, where $Y$ is:\n",
    "$$ Y = \\frac{\\sum_{i=1}^n X_i}{n}$$\n",
    "$$ Y \\sim N(0.95, \\frac{0.95(1-0.95)}{n})$$\n",
    "\n",
    "Confidence interval of $Y$ at 95% significance level will be:\n",
    "$$ ( 0.95 \\pm Z_{\\frac{\\alpha}{2}}\\sigma_Y)$$\n",
    "With 20 confidence interval estimated:\n",
    "$$ ( 0.95 \\pm 1.96\\sqrt{\\frac{0.95(1-0.95)}{20}})$$\n",
    "The interval is:\n",
    "$$ ( 0.95 \\pm 1.96\\sqrt{\\frac{0.95(1-0.95)}{20}})$$\n",
    "$$ (0.85448, 1.0455)$$\n",
    "\n",
    "This can be intepreted as: at minimum, there is a probability of 85.448% that a confidence interval is correct. With 20 confidence interval taken as sample, it is expected to see $(0.85448 * 20) \\approx 17$ to be correct.\n",
    "Which I will be suprosed if only 16 of them are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (5 marks)\n",
    "\n",
    "You have answered the questions from Professor Ozstraya’s AI. Now with the knowledge of the central limit theorem, you want to find the minimum number of confidence intervals that guarantees, the probabilty of the difference between the confidence level from the empirical result (e.g., one possibility is $\\frac{16}{20}=0.8$ for 16 correct of 20 total intervals) and the theoretical confidence level (assume 0.95 for a 95% confidence level as in the previous question) being more than 0.05, will be less than 0.01.\n",
    "\n",
    "In more precise terms, let's denote the event as the correctness of each confidence interval using random variables $X_1, X_2,..., X_n \\sim \\text{Ber}(\\theta)$. 1 for correct, 0 for wrong. Using the information in the previous paragraph, calculate the smallest number of confidence intervals, $\\textbf{n}$, you have to observe to guarantee that the probability of the difference between the confidence level from the empirical result and the theoretical confidence level being more than 0.05, will be less than 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "\n",
    "Given:\n",
    "$$ X \\sim Be(0.8)$$\n",
    "Under CLT, it could be converted as a normal distribution as:\n",
    "$$ \\hat \\theta = \\frac{\\sum_{i=1}^n X_i}{n}$$\n",
    "$$\\hat \\theta \\sim N\\left(\\theta, \\frac{\\theta(1-\\theta)}{n}\\right)$$\n",
    "\n",
    "Given empirical result $\\theta = 0.8$, and its estimate as $\\hat \\theta$ to find the difference stated in the question, the following inequality could be set up:\n",
    "$$ P(\\hat \\theta - \\theta > 0.05) < 0.01$$\n",
    "\n",
    "Furthermore, the distribution could be converted as a standard normal distribution $Z_{\\hat\\theta}$:\n",
    "$$Z_{\\hat\\theta} = \\frac{\\hat\\theta - \\theta}{\\sqrt{\\frac{\\theta(1-\\theta)}{n}}}$$\n",
    "\n",
    "Hence, the inequality could be rewrote as:\n",
    "$$ P\\left(\\frac{\\hat\\theta - \\theta}{\\sqrt{\\frac{\\theta(1-\\theta)}{n}}} > \\frac{0.05}{\\sqrt{\\frac{\\theta(1-\\theta)}{n}}}\\right) < 0.01$$\n",
    "that is:\n",
    "$$ P\\left(Z_{\\hat\\theta} > \\frac{0.05}{\\sqrt{\\frac{\\theta(1-\\theta)}{n}}}\\right) < 0.01$$\n",
    "Apply that\n",
    "$$ P( X > x) = 1 - P( X < x)$$\n",
    "$$ P\\left(Z_{\\hat\\theta} < \\frac{0.05}{\\sqrt{\\frac{\\theta(1-\\theta)}{n}}}\\right) > 0.99$$\n",
    "Consider the distribution indicates an one-tail Z-distribution situation. By referring to the Z-table, the corresponding Z value is $2.33$, where $\\theta = 0.8$.\n",
    "Hence, it is obtained that:\n",
    "$$2.33 < \\frac{0.05}{\\sqrt{\\frac{0.8(1-0.8)}{n}}}$$\n",
    "$$n > \\left(\\frac{2.33\\sqrt{0.8(0.2)}}{0.05}\\right)^2$$\n",
    "$$n > 347.45$$\n",
    "Therefore, the smallest number of confidence intervals need to be observed to guarantee that the required probability is $348$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 Linear Regression - The Joy Prediction Challenge (45 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your analysis above Professor Ozstraya's AI reincarnated launches The Endeavour mission. Not only does it collect the critical predictor variables directly from Terra Australis, but it also collects the average joy and critical predictor variables from numerous planets around the universe that have been split into training and testing sets. This will make it possible for you to build and test a prediction model of average joy on a planet given the different predictor variables available on the same planet. It's just what you need to predict the average joy on Terra Australis.\n",
    "\n",
    "Noting that the average joy on Terra Australis is likely to follow a Gaussian distribution based on your analysis above, you recall that the MLE solution for linear regression covered in Lecture 9 also assumes a Gaussian target. So you reason that linear regression would be a good place to start with building a predictor model.\n",
    "\n",
    "The datasets you are working with are ```Regression_train.csv```, ```Regression_test.csv```, ```Regression_new.csv```, and ```Regression_Terra_Australis.csv```. You can find them in the unit assessment webpage. **joyjoy** is the target variable, and others are predictors (attributes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (NO LIBRARIES ALLOWED) (1 Mark)\n",
    "The linear regression model is your starting point. As the linear regression model assumes that predictors are linearly correlated with the target. You want to calculate the correlation coefficient for each pair of predictor and target to learn which predictor(s) is(are) significantly linearly correlated with the target (use coefficient greater than 0.1 as a threshold). Please load ```Regression_train.csv``` and write an R script to automatically iterate through all predictors and print important predictors.\n",
    "**NOTE**: Manually doing the this task will result in 0 mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER BLOCK\n",
    "train<-read.csv(\"Regression_train.csv\")\n",
    "names(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_train <- lm(joyjoy ~., train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "est <- summary(lm_train)$coefficients[,1]\n",
    "est <- est[-1]\n",
    "est <- data.frame(est)\n",
    "colnames(est)  <- \"coefficients\"\n",
    "sub  <-  subset(est, abs(est$coefficients) > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Important predictor with coefficient greater than 0.1 is/are:  Cooperativebehavior .\"\n"
     ]
    }
   ],
   "source": [
    "print(paste(\"Important predictor with coefficient greater than 0.1 is/are: \",\n",
    "            paste(rownames(sub), \n",
    "                  paste(\".\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (NO LIBRARIES ALLOWED) (1 Mark)\n",
    "Please load the ```Regression_train.csv``` and fit a multiple linear regression model, called ```fit.cor```, with the predictors you found from Question 1. In addition, you want to fit another multiple linear regression model, called ```fit.all```, using all predictors. In terms of R-squared, which model looks better and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'call'</li>\n",
       "\t<li>'terms'</li>\n",
       "\t<li>'residuals'</li>\n",
       "\t<li>'coefficients'</li>\n",
       "\t<li>'aliased'</li>\n",
       "\t<li>'sigma'</li>\n",
       "\t<li>'df'</li>\n",
       "\t<li>'r.squared'</li>\n",
       "\t<li>'adj.r.squared'</li>\n",
       "\t<li>'fstatistic'</li>\n",
       "\t<li>'cov.unscaled'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'call'\n",
       "\\item 'terms'\n",
       "\\item 'residuals'\n",
       "\\item 'coefficients'\n",
       "\\item 'aliased'\n",
       "\\item 'sigma'\n",
       "\\item 'df'\n",
       "\\item 'r.squared'\n",
       "\\item 'adj.r.squared'\n",
       "\\item 'fstatistic'\n",
       "\\item 'cov.unscaled'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'call'\n",
       "2. 'terms'\n",
       "3. 'residuals'\n",
       "4. 'coefficients'\n",
       "5. 'aliased'\n",
       "6. 'sigma'\n",
       "7. 'df'\n",
       "8. 'r.squared'\n",
       "9. 'adj.r.squared'\n",
       "10. 'fstatistic'\n",
       "11. 'cov.unscaled'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n",
       " [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n",
       " [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER BLOCK\n",
    "# Fitting two models\n",
    "# Model that only include predictors with coefficients greater than 0.1\n",
    "fit.cor<- lm(joyjoy ~ Cooperativebehavior, train)\n",
    "\n",
    "# Model with all predictors\n",
    "fit.all<- lm(joyjoy ~., train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.0401566675459227"
      ],
      "text/latex": [
       "0.0401566675459227"
      ],
      "text/markdown": [
       "0.0401566675459227"
      ],
      "text/plain": [
       "[1] 0.04015667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get r-squared value\n",
    "fit_cor_r2 <- summary(fit.cor)$r.squared\n",
    "fit_cor_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.198523739482087"
      ],
      "text/latex": [
       "0.198523739482087"
      ],
      "text/markdown": [
       "0.198523739482087"
      ],
      "text/plain": [
       "[1] 0.1985237"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get r-squared value\n",
    "fit_all_r2 <- summary(fit.all)$r.squared\n",
    "fit_all_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER (TEXT)\n",
    "As there is only one predictor has a coefficient greater than the threshold value $0.1$. Hence, for the model $fit.cor$, has only one predictor, named Cooperativebehavior, included. The R-squared value resulted from the model was $0.0401566675459227$.\n",
    "\n",
    "For the other model, which included all predictors, its R-squared value was $0.198523739482087$. The result is significantly better than the first model which only included one predictor.\n",
    "\n",
    "Consider R-squared value indicate how well the model fit the data points. The values could be intepreted that the first model with only predictor is not enough to explain the target of the model, \"joyjoy\", which is the average joy level. Situation of underfitting could have occured for the first model.\n",
    "\n",
    "However, the second model is not perfect although it resulted in a significantly higher value of R-squared. By inputting all predictors, situation of overfitting might occur, which the model's regression line simply follows all data points instead of giving a trend of all points.\n",
    "\n",
    "Anyhow, by comparing these two model, the models with all predictor still outperformed the one with only one predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (NO LIBRARIES ALLOWED) (1 Mark)\n",
    "According to the summary table of ```fit.all```, which predictors do you think are possibly associated with the target variable (use the significance level of $0.001$), and which are the **Top 5** strongest predictors? Please write an R script to automatically fetch and print this information. **NOTE**: Manually doing this task will result in 0 mark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The important features are:  Cooperativebehavior; Socialadaptation; Appropriatesocialskills; PositiveSymptomDistressIndex.PSDI.; School.academicproblems; Interpersonalsensitivity; Anxiety; Psychosomaticdisorders; General.SymptomaticIndex.GSI.; Shyness.withdrawal; Behavioralproblemstotal; Psychoticism; Phobicanxiety; Somatization; Anxietyproblems; Self.concept; Depression; Self.esteem; Jealousy.withdrawal; PositiveSymptomTotal.PST.; Additional; Obsessionâ..compulsion; Hostility; Paranoidideation; Negativesocialskillstotal; Overconfidence; Impulsiveness\"\n",
      "[1] \"The top 5 most important features are:  Cooperativebehavior; Socialadaptation; Appropriatesocialskills; PositiveSymptomDistressIndex.PSDI.; School.academicproblems\"\n"
     ]
    }
   ],
   "source": [
    "top_predictors <- function(summary){\n",
    "    p_val  <- summary$coefficients[, 4]\n",
    "    p_val <- sort(p_val, decreasing = FALSE)\n",
    "    p_val <- p_val[-1]\n",
    "    p_val <- data.frame(p_val)\n",
    "    colnames(p_val) <- \"p_value\"\n",
    "    sub <- subset(p_val, p_val$p_value < 0.001)\n",
    "    coef.most  <- rownames(head(sub,5))\n",
    "    coef.imp <- rownames(sub)\n",
    "    print(paste(\"The important features are: \",paste(coef.imp, collapse = \"; \")))\n",
    "    print(paste(\"The top 5 most important features are: \",paste(coef.most, collapse = \"; \")))\n",
    "}\n",
    "top_predictors(summary(fit.all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coef <- fit.all$coefficients\n",
    "# p_val <- sort(abs(coef), decreasing = TRUE)\n",
    "# head(p_val[2:31],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (NO LIBRARIES ALLOWED) (2 Marks)\n",
    "Rather than calling the ```lm()``` function, you would like to write your own function to do the [least square](https://en.wikipedia.org/wiki/Least_squares) estimation for the simple linear regression model parameters $\\boldsymbol{\\beta_0}$ and $\\boldsymbol{\\beta_1}$. The function takes two input arguments with the first being the dataframe name and the second the predictor name, and outputs the fitted linear model with the form: $$\\textbf{E}[\\text{average joy}|X]=\\hat{\\beta}_{0}+\\hat{\\beta}{_1}x$$ \n",
    "\n",
    "Code up this function in R and apply it to the  predictor **Negativesocialskillstotal**, and explain the effect that this variable has on **joyjoy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (i in 1:length(names(train))){\n",
    "#     print(names(train[,i]))\n",
    "# }\n",
    "names(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"E[joyjoy]=39.5508864735738+0.0262173630861777*Negativesocialskillstotal\"\n"
     ]
    }
   ],
   "source": [
    "# ANSWER BLOCK\n",
    "least_square <- function(df, p){ # df is the dataframe; p is the predictor name\n",
    "    ## implement the least square estimator here\n",
    "    SS_xx <- sum((df[, p] - mean(df[, p]))^2)\n",
    "    SS_xy <- sum( (df[, p] - mean(df[, p]))*(df[,\"joyjoy\"] - mean(df[, \"joyjoy\"])) ) \n",
    "    beta_1_hat <- SS_xy / SS_xx\n",
    "    beta_0_hat <- mean(df[, \"joyjoy\"]) - beta_1_hat * mean(df[, p])\n",
    "    print(paste0('E[joyjoy]=',beta_0_hat,'+',beta_1_hat,'*', p))  \n",
    "}\n",
    "least_square(train, \"Negativesocialskillstotal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (NO LIBRARIES ALLOWED) (1 Mark)\n",
    "[**R squared**](https://en.wikipedia.org/wiki/Coefficient_of_determination) from the summary table reflects that the full model doesn't fit the training dataset well; thus, you try to quantify the error between the ground-truth **joyjoy** and the model prediction. You want to write a function to predict **joyjoy** with the given dataframe and model, and calculate the [root mean squared error (rMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) between the model predictions and the ground truths. Please test this function on the model ```fit.all``` and the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "15.7504083502128"
      ],
      "text/latex": [
       "15.7504083502128"
      ],
      "text/markdown": [
       "15.7504083502128"
      ],
      "text/plain": [
       "[1] 15.75041"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER BLOCK\n",
    "rmse <- function(df,model){\n",
    "    ## implement R squared here\n",
    "    prediction <- predict(model, df)\n",
    "    return(sqrt(sum((prediction - df[,'joyjoy'])^2)/nrow(df)))\n",
    "}\n",
    "rmse(train, fit.all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (NO LIBRARIES ALLOWED) (1 Mark)\n",
    "You have been given a new dataset ```Regression_new.csv```. You are going to apply the model ```fit.all``` on the new dataset to evaluate the model performance using **rMSE**. When you look into **rMSE**, what do you find? And do you think the model works equivalently well on ```Regression_train.csv``` and ```Regression_new.csv``` and why? Can you point out potential reason(s) for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "27.6586070094456"
      ],
      "text/latex": [
       "27.6586070094456"
      ],
      "text/markdown": [
       "27.6586070094456"
      ],
      "text/plain": [
       "[1] 27.65861"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER BLOCK\n",
    "new_data <- read.csv(\"Regression_new.csv\")\n",
    "rmse(new_data, fit.all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 (NO LIBRARIES ALLOWED) (1 Mark)\n",
    "You find the full model complicated and try to reduce the complexity by performing [bidirectional stepwise regression](https://en.wikipedia.org/wiki/Stepwise_regression) with BIC.\n",
    "\n",
    "Calculate the **rMSE** of this new model from the training data with the function that you implemented above. Explain your findings within 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER BLOCK\n",
    "fit.step <- step(fit.all, direction = \"both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 (Libraries are allowed) (35 Marks)\n",
    "As a Data Scientist, one of the key tasks is to build models that can predict the target precisely; thus, modelling will not be limited to the aforementioned steps in this assignment. To simulate for a realistic modelling process, this question will be in the form of a [Kaggle competition called the \"Bring Joy to the Universe Challenge\"](https://www.kaggle.com/t/b1d5a5a9b01848bc991b66c3fb12422d) among students to find out who has the best model.\n",
    "\n",
    "Thus, you **will be graded by the rMSE** performance of your model, the better your model, the higher your score. Additionally, you need to describe/document your thought process in this model building process, this is akin to showing your working properly for the mathematic sections. If you don't clearly document the reasonings behind the model you use, we will have to make some deductions on your scores.\n",
    "\n",
    "This is the [video tutorial](https://www.youtube.com/watch?v=rkXc25Uvyl4) on how to join any Kaggle competition. \n",
    "\n",
    "When you optimize your model's performance, you can use any models that you know and feature selection might be a big help as well. [Check the non-exhaustive set of R functions relevant to this unit](https://lms.monash.edu/mod/resource/view.php?id=10554921) for ideas for different models to try.\n",
    "\n",
    "$\\textbf{Note}$ Please make sure that we can install the libraries that you use in this part, the code structure can be:\n",
    "\n",
    "```install.packages(\"some package\", repos='http://cran.us.r-project.org')```\n",
    "\n",
    "```library(\"some package\")```\n",
    "\n",
    "Remember that if we cannot run your code, we will have to give you a deduction. Our suggestion is for you to use the standard ```R version 3.6.1```\n",
    "\n",
    "You also need to name your final model ``fin.mod`` so we can run a check to find out your performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGboost\n",
    "# install.packages(\"xgboost\")\n",
    "# library(\"xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# install.packages(\"caret\", dependencies = c(\"Depends\", \"Imports\", \"Suggests\"))\n",
    "# library(\"caret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain <- read.csv(\"Regression_train.csv\")\n",
    "para <- as.matrix(dtrain[,names(dtrain) != \"joyjoy\"])\n",
    "label <-  as.matrix(dtrain$joyjoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>PositiveSymptomDistressIndex.PSDI.</dt>\n",
       "\t\t<dd>-0.0599406594186315</dd>\n",
       "\t<dt>School.academicproblems</dt>\n",
       "\t\t<dd>-0.0576776298359663</dd>\n",
       "\t<dt>Interpersonalsensitivity</dt>\n",
       "\t\t<dd>-0.0569572320556571</dd>\n",
       "\t<dt>Anxiety</dt>\n",
       "\t\t<dd>-0.0564389387045</dd>\n",
       "\t<dt>Psychosomaticdisorders</dt>\n",
       "\t\t<dd>-0.0561956519845266</dd>\n",
       "\t<dt>General.SymptomaticIndex.GSI.</dt>\n",
       "\t\t<dd>-0.0552286083419373</dd>\n",
       "\t<dt>Shyness.withdrawal</dt>\n",
       "\t\t<dd>-0.0549667301006633</dd>\n",
       "\t<dt>Psychoticism</dt>\n",
       "\t\t<dd>-0.0542152229224718</dd>\n",
       "\t<dt>Behavioralproblemstotal</dt>\n",
       "\t\t<dd>-0.054049515492903</dd>\n",
       "\t<dt>Phobicanxiety</dt>\n",
       "\t\t<dd>-0.052103905959668</dd>\n",
       "\t<dt>Somatization</dt>\n",
       "\t\t<dd>-0.0514578230142474</dd>\n",
       "\t<dt>Anxietyproblems</dt>\n",
       "\t\t<dd>-0.0511187944816276</dd>\n",
       "\t<dt>Overconfidence</dt>\n",
       "\t\t<dd>-0.0190812691236063</dd>\n",
       "\t<dt>Antisocialbehavior</dt>\n",
       "\t\t<dd>-0.00459670032717745</dd>\n",
       "\t<dt>Psychopathologicaldisorders</dt>\n",
       "\t\t<dd>0.00619477550766123</dd>\n",
       "\t<dt>Inappropriateassertiveness</dt>\n",
       "\t\t<dd>0.00691166281074053</dd>\n",
       "\t<dt>Impulsiveness</dt>\n",
       "\t\t<dd>0.0174416271042323</dd>\n",
       "\t<dt>Negativesocialskillstotal</dt>\n",
       "\t\t<dd>0.0256277732685659</dd>\n",
       "\t<dt>Paranoidideation</dt>\n",
       "\t\t<dd>0.0264330983217856</dd>\n",
       "\t<dt>Hostility</dt>\n",
       "\t\t<dd>0.0274781402034457</dd>\n",
       "\t<dt>Obsessionâ..compulsion</dt>\n",
       "\t\t<dd>0.0320701087036187</dd>\n",
       "\t<dt>Additional</dt>\n",
       "\t\t<dd>0.0347944874138949</dd>\n",
       "\t<dt>PositiveSymptomTotal.PST.</dt>\n",
       "\t\t<dd>0.0403894808253091</dd>\n",
       "\t<dt>Jealousy.withdrawal</dt>\n",
       "\t\t<dd>0.0408225700142611</dd>\n",
       "\t<dt>Self.esteem</dt>\n",
       "\t\t<dd>0.0438455583924144</dd>\n",
       "\t<dt>Depression</dt>\n",
       "\t\t<dd>0.0447040656572435</dd>\n",
       "\t<dt>Self.concept</dt>\n",
       "\t\t<dd>0.0486172495118642</dd>\n",
       "\t<dt>Appropriatesocialskills</dt>\n",
       "\t\t<dd>0.0718566989109768</dd>\n",
       "\t<dt>Socialadaptation</dt>\n",
       "\t\t<dd>0.0831281371566765</dd>\n",
       "\t<dt>Cooperativebehavior</dt>\n",
       "\t\t<dd>0.128061527668453</dd>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>41.1131701046695</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[PositiveSymptomDistressIndex.PSDI.] -0.0599406594186315\n",
       "\\item[School.academicproblems] -0.0576776298359663\n",
       "\\item[Interpersonalsensitivity] -0.0569572320556571\n",
       "\\item[Anxiety] -0.0564389387045\n",
       "\\item[Psychosomaticdisorders] -0.0561956519845266\n",
       "\\item[General.SymptomaticIndex.GSI.] -0.0552286083419373\n",
       "\\item[Shyness.withdrawal] -0.0549667301006633\n",
       "\\item[Psychoticism] -0.0542152229224718\n",
       "\\item[Behavioralproblemstotal] -0.054049515492903\n",
       "\\item[Phobicanxiety] -0.052103905959668\n",
       "\\item[Somatization] -0.0514578230142474\n",
       "\\item[Anxietyproblems] -0.0511187944816276\n",
       "\\item[Overconfidence] -0.0190812691236063\n",
       "\\item[Antisocialbehavior] -0.00459670032717745\n",
       "\\item[Psychopathologicaldisorders] 0.00619477550766123\n",
       "\\item[Inappropriateassertiveness] 0.00691166281074053\n",
       "\\item[Impulsiveness] 0.0174416271042323\n",
       "\\item[Negativesocialskillstotal] 0.0256277732685659\n",
       "\\item[Paranoidideation] 0.0264330983217856\n",
       "\\item[Hostility] 0.0274781402034457\n",
       "\\item[Obsessionâ..compulsion] 0.0320701087036187\n",
       "\\item[Additional] 0.0347944874138949\n",
       "\\item[PositiveSymptomTotal.PST.] 0.0403894808253091\n",
       "\\item[Jealousy.withdrawal] 0.0408225700142611\n",
       "\\item[Self.esteem] 0.0438455583924144\n",
       "\\item[Depression] 0.0447040656572435\n",
       "\\item[Self.concept] 0.0486172495118642\n",
       "\\item[Appropriatesocialskills] 0.0718566989109768\n",
       "\\item[Socialadaptation] 0.0831281371566765\n",
       "\\item[Cooperativebehavior] 0.128061527668453\n",
       "\\item[(Intercept)] 41.1131701046695\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "PositiveSymptomDistressIndex.PSDI.\n",
       ":   -0.0599406594186315School.academicproblems\n",
       ":   -0.0576776298359663Interpersonalsensitivity\n",
       ":   -0.0569572320556571Anxiety\n",
       ":   -0.0564389387045Psychosomaticdisorders\n",
       ":   -0.0561956519845266General.SymptomaticIndex.GSI.\n",
       ":   -0.0552286083419373Shyness.withdrawal\n",
       ":   -0.0549667301006633Psychoticism\n",
       ":   -0.0542152229224718Behavioralproblemstotal\n",
       ":   -0.054049515492903Phobicanxiety\n",
       ":   -0.052103905959668Somatization\n",
       ":   -0.0514578230142474Anxietyproblems\n",
       ":   -0.0511187944816276Overconfidence\n",
       ":   -0.0190812691236063Antisocialbehavior\n",
       ":   -0.00459670032717745Psychopathologicaldisorders\n",
       ":   0.00619477550766123Inappropriateassertiveness\n",
       ":   0.00691166281074053Impulsiveness\n",
       ":   0.0174416271042323Negativesocialskillstotal\n",
       ":   0.0256277732685659Paranoidideation\n",
       ":   0.0264330983217856Hostility\n",
       ":   0.0274781402034457Obsessionâ..compulsion\n",
       ":   0.0320701087036187Additional\n",
       ":   0.0347944874138949PositiveSymptomTotal.PST.\n",
       ":   0.0403894808253091Jealousy.withdrawal\n",
       ":   0.0408225700142611Self.esteem\n",
       ":   0.0438455583924144Depression\n",
       ":   0.0447040656572435Self.concept\n",
       ":   0.0486172495118642Appropriatesocialskills\n",
       ":   0.0718566989109768Socialadaptation\n",
       ":   0.0831281371566765Cooperativebehavior\n",
       ":   0.128061527668453(Intercept)\n",
       ":   41.1131701046695\n",
       "\n"
      ],
      "text/plain": [
       "PositiveSymptomDistressIndex.PSDI.            School.academicproblems \n",
       "                      -0.059940659                       -0.057677630 \n",
       "          Interpersonalsensitivity                            Anxiety \n",
       "                      -0.056957232                       -0.056438939 \n",
       "            Psychosomaticdisorders      General.SymptomaticIndex.GSI. \n",
       "                      -0.056195652                       -0.055228608 \n",
       "                Shyness.withdrawal                       Psychoticism \n",
       "                      -0.054966730                       -0.054215223 \n",
       "           Behavioralproblemstotal                      Phobicanxiety \n",
       "                      -0.054049515                       -0.052103906 \n",
       "                      Somatization                    Anxietyproblems \n",
       "                      -0.051457823                       -0.051118794 \n",
       "                    Overconfidence                 Antisocialbehavior \n",
       "                      -0.019081269                       -0.004596700 \n",
       "       Psychopathologicaldisorders         Inappropriateassertiveness \n",
       "                       0.006194776                        0.006911663 \n",
       "                     Impulsiveness          Negativesocialskillstotal \n",
       "                       0.017441627                        0.025627773 \n",
       "                  Paranoidideation                          Hostility \n",
       "                       0.026433098                        0.027478140 \n",
       "            Obsessionâ..compulsion                         Additional \n",
       "                       0.032070109                        0.034794487 \n",
       "         PositiveSymptomTotal.PST.                Jealousy.withdrawal \n",
       "                       0.040389481                        0.040822570 \n",
       "                       Self.esteem                         Depression \n",
       "                       0.043845558                        0.044704066 \n",
       "                      Self.concept            Appropriatesocialskills \n",
       "                       0.048617250                        0.071856699 \n",
       "                  Socialadaptation                Cooperativebehavior \n",
       "                       0.083128137                        0.128061528 \n",
       "                       (Intercept) \n",
       "                      41.113170105 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sort(fit.all$coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops  <-  c(\"Antisocialbehavior\", \"Psychopathologicaldisorders\", \"Inappropriateassertiveness\", \"Impulsiveness\",\n",
    "            \"Overconfidence\", \"Negativesocialskillstotal\")\n",
    "train <- train[, !(names(train) %in% drops)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in corrplot(correlations, method = \"circle\", type = \"lower\", sig.level = 0.01, : could not find function \"corrplot\"\n",
     "output_type": "error",
     "traceback": [
      "Error in corrplot(correlations, method = \"circle\", type = \"lower\", sig.level = 0.01, : could not find function \"corrplot\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "correlations <- cor(train, use = 'everything')\n",
    "corrplot(correlations, method = 'circle', type = 'lower', sig.level = 0.01, insig ='blank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = list(\n",
    "  objective = \"regression\"\n",
    "  , metric = \"30\"\n",
    "  , min_data = 1L\n",
    "  , learning_rate = .2\n",
    ")\n",
    "train_x <- train[,names(train) != \"joyjoy\"]\n",
    "train_y <- train$joyjoy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in train(joyjoy ~ ., data = dtrain, method = \"lasso\"): could not find function \"train\"\n",
     "output_type": "error",
     "traceback": [
      "Error in train(joyjoy ~ ., data = dtrain, method = \"lasso\"): could not find function \"train\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "fin.mod <- lightgbm(data = as.matrix(train_x),\n",
    "                params = train_params,\n",
    "                label = as.matrix(train_y), \n",
    "                nrounds = 20\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.mod <- xgboost(data = para, label = label, nround = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para[,]\n",
    "# dtrain = xgb.DMatrix(as.matrix(sapply(dtrain, as.numeric)), label=dtrain$joyjoy)\n",
    "\n",
    "# fin.mod <- xgboost(data = dtrain[,names(dtrain) != \"joyjoy\"], label = dtrain$joyjoy, max.depth = 2, eta = 1, nthread = 2, nrounds = 2)\n",
    "# params <- list(booster = \"gbtree\", objective = \"reg:squarederror\", eta = 0.005, gamma = 0, max_depth = 60,\n",
    "#               min_child_weight = 1, subsample = 1, colsample_bytree = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your final model here, use additional coding blocks if you need to\n",
    "# fin.mod <- NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the test data.\n",
    "test <- read.csv(\"Regression_test.csv\")\n",
    "# If you are using any packages that perform the prediction differently, please change this line of code accordingly.\n",
    "pred.label <- predict(fin.mod, test)\n",
    "# put these predicted labels in a csv file that you can use to commit to the Kaggle Leaderboard\n",
    "write.csv(data.frame(\"RowIndex\" = seq(1, length(pred.label)), \"Prediction\" = pred.label),  \n",
    "          \"RegressionPredictLabel.csv\", row.names = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(pred.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK, YOU ARE REQUIRED TO HAVE THIS CODE BLOCK IN YOUR JUPYTER NOTEBOOK SUBMISSION\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "\n",
    "tryCatch(\n",
    "    {\n",
    "        source(\"../supplimentary.R\")\n",
    "    },\n",
    "    error = function(e){\n",
    "        source(\"supplimentary.R\")\n",
    "    }\n",
    ")\n",
    "\n",
    "truths <- tryCatch(\n",
    "    {\n",
    "        read.csv(\"../Regression_truths.csv\")\n",
    "    },\n",
    "    error = function(e){\n",
    "        read.csv(\"Regression_truths.csv\")\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "RMSE.fin <- rmse(pred.label, truths$Label)\n",
    "cat(paste(\"RMSE is\", RMSE.fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 (Libraries are allowed) (2 Marks)\n",
    "Use your model in question 8 to predict the average joy of killer drop bears on Terra Australis using the file ```Regression_Terra_Australis.csv``` which contains the predictor values from Terra Australis. Are killer drop bears on Terra Australis joyful? Based on your model, what predictors could be changed and how, in order to increase the joy of killer drop bears and bring joy to the universe? Answer with less than 100 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Terra Australis test data.\n",
    "test <- read.csv(\"Regression_Terra_Australis.csv\")\n",
    "# If you are using any packages that perform the prediction differently, please change this line of code accordingly.\n",
    "pred.label <- predict(fin.mod, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER BLOCK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER (TEXT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
